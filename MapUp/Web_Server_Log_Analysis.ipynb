{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8WCGUqi32327m8l6AKPy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrudulamadhavan/My-Projects/blob/main/MapUp/Web_Server_Log_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3551217"
      },
      "source": [
        "# Web Server Log Analysis - Python Take-Home Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98eb0efa"
      },
      "source": [
        "## Overview\n",
        "This assessment involves analyzing the Calgary HTTP dataset, which contains approximately one year's worth of HTTP requests to the University of Calgary's Computer Science web server. You'll work with real-world web server log data to extract meaningful insights and demonstrate your Python data analysis skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81debeba"
      },
      "source": [
        "## Part 1: Data Loading and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget ftp://ita.ee.lbl.gov/traces/calgary_access_log.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H5lCMJhNly1",
        "outputId": "6213f1cb-6e08-4d0c-aa01-834a012a7f67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 17:41:09--  ftp://ita.ee.lbl.gov/traces/calgary_access_log.gz\n",
            "           => ‘calgary_access_log.gz’\n",
            "Resolving ita.ee.lbl.gov (ita.ee.lbl.gov)... 131.243.2.164, 2620:83:8000:102::a4\n",
            "Connecting to ita.ee.lbl.gov (ita.ee.lbl.gov)|131.243.2.164|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /traces ... done.\n",
            "==> SIZE calgary_access_log.gz ... 5435681\n",
            "==> PASV ... done.    ==> RETR calgary_access_log.gz ... done.\n",
            "Length: 5435681 (5.2M) (unauthoritative)\n",
            "\n",
            "calgary_access_log. 100%[===================>]   5.18M  1.33MB/s    in 3.9s    \n",
            "\n",
            "2025-06-04 17:41:15 (1.33 MB/s) - ‘calgary_access_log.gz’ saved [5435681]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('calgary_access_log_analysis').getOrCreate()\n",
        "print(f'spark version: {spark.version}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tetOrg9rNlrF",
        "outputId": "4b3aacd8-182f-4e42-c4fa-4033208bc66e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_file_path=\"calgary_access_log.gz\"\n",
        "raw_data = spark.read.text(log_file_path)"
      ],
      "metadata": {
        "id": "QRgSXLCVUGN3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract, col, to_timestamp, when, split\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Regular expressions for log parsing\n",
        "host_pattern = r'^(\\S+)'  # First token\n",
        "timestamp_pattern = r'\\[(.*?)\\]'  # Between square brackets\n",
        "method_endpoint_pattern = r'\"(GET|POST|HEAD|PUT|DELETE)\\s+(\\S+)'  # HTTP method + endpoint\n",
        "status_pattern = r'\"\\s(\\d{3})\\s'  # HTTP status\n",
        "size_pattern = r'\\s(\\d+|-)$'  # Last token is content size or \"-\"\n",
        "\n",
        "# Extract structured columns from raw log lines\n",
        "logs_df = (\n",
        "    raw_data\n",
        "    .withColumn(\"host\", regexp_extract(\"value\", host_pattern, 1))\n",
        "    .withColumn(\"timestamp_str\", regexp_extract(\"value\", timestamp_pattern, 1))\n",
        "    .withColumn(\"method\", regexp_extract(\"value\", method_endpoint_pattern, 1))\n",
        "    .withColumn(\"endpoint\", regexp_extract(\"value\", method_endpoint_pattern, 2))\n",
        "    .withColumn(\"file_extension\", regexp_extract(\"endpoint\", r'\\.([a-zA-Z0-9]+)$', 1))\n",
        "    .withColumn(\"status\", regexp_extract(\"value\", status_pattern, 1).cast(IntegerType()))\n",
        "    .withColumn(\"content_size_raw\", regexp_extract(\"value\", size_pattern, 1))\n",
        ")\n",
        "\n",
        "\n",
        "logs_df.show(5,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S70h7aMYUGKo",
        "outputId": "f044a45c-f7fe-45aa-9ac7-4401b0b770ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------+-----+--------------------------+------+----------+--------------+------+----------------+\n",
            "|value                                                                    |host |timestamp_str             |method|endpoint  |file_extension|status|content_size_raw|\n",
            "+-------------------------------------------------------------------------+-----+--------------------------+------+----------+--------------+------+----------------+\n",
            "|local - - [24/Oct/1994:13:41:41 -0600] \"GET index.html HTTP/1.0\" 200 150 |local|24/Oct/1994:13:41:41 -0600|GET   |index.html|html          |200   |150             |\n",
            "|local - - [24/Oct/1994:13:41:41 -0600] \"GET 1.gif HTTP/1.0\" 200 1210     |local|24/Oct/1994:13:41:41 -0600|GET   |1.gif     |gif           |200   |1210            |\n",
            "|local - - [24/Oct/1994:13:43:13 -0600] \"GET index.html HTTP/1.0\" 200 3185|local|24/Oct/1994:13:43:13 -0600|GET   |index.html|html          |200   |3185            |\n",
            "|local - - [24/Oct/1994:13:43:14 -0600] \"GET 2.gif HTTP/1.0\" 200 2555     |local|24/Oct/1994:13:43:14 -0600|GET   |2.gif     |gif           |200   |2555            |\n",
            "|local - - [24/Oct/1994:13:43:15 -0600] \"GET 3.gif HTTP/1.0\" 200 36403    |local|24/Oct/1994:13:43:15 -0600|GET   |3.gif     |gif           |200   |36403           |\n",
            "+-------------------------------------------------------------------------+-----+--------------------------+------+----------+--------------+------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2icu9a9zanio",
        "outputId": "cd930fb5-f052-4367-d3a5-97d67e8b50a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            " |-- host: string (nullable = true)\n",
            " |-- timestamp_str: string (nullable = true)\n",
            " |-- method: string (nullable = true)\n",
            " |-- endpoint: string (nullable = true)\n",
            " |-- file_extension: string (nullable = true)\n",
            " |-- status: integer (nullable = true)\n",
            " |-- content_size_raw: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of rows\n",
        "num_rows = logs_df.count()\n",
        "\n",
        "# Number of columns\n",
        "num_cols = len(logs_df.columns)\n",
        "\n",
        "print(f\"Rows: {num_rows}, Columns: {num_cols}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHVQEFDfkoLM",
        "outputId": "93e96201-e1ad-4326-f139-9dff10efa2bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 726739, Columns: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary\n",
        "logs_df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFt7XG2MkoI5",
        "outputId": "b18a398e-8570-4743-f3de-b1b72663b925"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+------+--------------------+------+------------------+------------------+------------------+------------------+\n",
            "|summary|               value|  host|       timestamp_str|method|          endpoint|    file_extension|            status|  content_size_raw|\n",
            "+-------+--------------------+------+--------------------+------+------------------+------------------+------------------+------------------+\n",
            "|  count|              726739|726739|              726739|726739|            726739|            726739|            724991|            726739|\n",
            "|   mean|                NULL|  NULL|                NULL|  NULL|       5330.885375|18.542857142857144|226.31063558030377|11914.615843876227|\n",
            "| stddev|                NULL|  NULL|                NULL|  NULL|3993.0734647323266| 91.96013933705598|53.286361129344805|103570.74453754502|\n",
            "|    min|local      index....| local|                    |      |                  |                  |               200|                  |\n",
            "|    max|remote HTTP/1.0\" ...|remote|31/Oct/1994:23:52...|  POST|        index.html|               zip|               501|              9995|\n",
            "+-------+--------------------+------+--------------------+------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "logs_df = logs_df.withColumn(\"timestamp\",to_timestamp(\"timestamp_str\", \"dd/MMM/yyyy:HH:mm:ss Z\"))"
      ],
      "metadata": {
        "id": "etzdNPHVgD74"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs_df.select(\"timestamp_str\", \"timestamp\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb_SXieVgD4V",
        "outputId": "d2d2b009-5397-4b6f-b0f7-f519fe5995f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-------------------+\n",
            "|timestamp_str             |timestamp          |\n",
            "+--------------------------+-------------------+\n",
            "|24/Oct/1994:13:41:41 -0600|1994-10-24 19:41:41|\n",
            "|24/Oct/1994:13:41:41 -0600|1994-10-24 19:41:41|\n",
            "|24/Oct/1994:13:43:13 -0600|1994-10-24 19:43:13|\n",
            "|24/Oct/1994:13:43:14 -0600|1994-10-24 19:43:14|\n",
            "|24/Oct/1994:13:43:15 -0600|1994-10-24 19:43:15|\n",
            "+--------------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean content size: convert to int, handle \"-\"\n",
        "logs_df = logs_df.withColumn(\"content_size\",when(col(\"content_size_raw\") == \"-\", None).otherwise(col(\"content_size_raw\").cast(IntegerType())))"
      ],
      "metadata": {
        "id": "3y5N81nbiBx4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs_df = logs_df.drop(\"timestamp_str\",\"content_size_raw\",\"value\")"
      ],
      "metadata": {
        "id": "Qh6mn-dnkoFH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs_df.select(\"host\", \"timestamp\", \"method\", \"endpoint\", \"file_extension\", \"status\", \"content_size\").show(15, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEvE9QjwsIu1",
        "outputId": "8adddf9c-cef8-4c2c-9407-7c3c470e8462"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+------+----------+--------------+------+------------+\n",
            "|host |timestamp          |method|endpoint  |file_extension|status|content_size|\n",
            "+-----+-------------------+------+----------+--------------+------+------------+\n",
            "|local|1994-10-24 19:41:41|GET   |index.html|html          |200   |150         |\n",
            "|local|1994-10-24 19:41:41|GET   |1.gif     |gif           |200   |1210        |\n",
            "|local|1994-10-24 19:43:13|GET   |index.html|html          |200   |3185        |\n",
            "|local|1994-10-24 19:43:14|GET   |2.gif     |gif           |200   |2555        |\n",
            "|local|1994-10-24 19:43:15|GET   |3.gif     |gif           |200   |36403       |\n",
            "|local|1994-10-24 19:43:17|GET   |4.gif     |gif           |200   |441         |\n",
            "|local|1994-10-24 19:46:45|GET   |index.html|html          |200   |3185        |\n",
            "|local|1994-10-24 19:46:45|GET   |2.gif     |gif           |200   |2555        |\n",
            "|local|1994-10-24 19:46:47|GET   |3.gif     |gif           |200   |36403       |\n",
            "|local|1994-10-24 19:46:50|GET   |4.gif     |gif           |200   |441         |\n",
            "|local|1994-10-24 19:47:19|GET   |index.html|html          |200   |150         |\n",
            "|local|1994-10-24 19:47:19|GET   |1.gif     |gif           |200   |1210        |\n",
            "|local|1994-10-24 19:47:41|GET   |index.html|html          |200   |3185        |\n",
            "|local|1994-10-24 19:47:50|GET   |5.html    |html          |200   |2065        |\n",
            "|local|1994-10-24 19:48:11|GET   |6.html    |html          |200   |1124        |\n",
            "+-----+-------------------+------+----------+--------------+------+------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, create_map, lit\n",
        "from itertools import chain\n",
        "\n",
        "# Step 1: List of valid file extensions\n",
        "valid_extensions = [\n",
        "    \"html\", \"htm\", \"jpg\", \"jpeg\", \"png\", \"gif\", \"css\", \"js\", \"json\",\n",
        "    \"xml\", \"txt\", \"pdf\", \"doc\", \"docx\", \"xls\", \"xlsx\", \"mp3\", \"wav\",\n",
        "    \"mp4\", \"avi\", \"mov\", \"zip\", \"rar\", \"tar\", \"gz\", \"svg\", \"ico\", \"bmp\",\n",
        "    \"tiff\", \"ps\", \"pcx\", \"rgb\", \"qt\"\n",
        "]\n",
        "\n",
        "# Step 2: Dictionary of known typos and strange values\n",
        "corrections = {\n",
        "    \"jpeg1\": \"jpeg\",\n",
        "    \"htmls\": \"html\",\n",
        "    \"hthml\": \"html\",\n",
        "    \"telnet\": \"unknown\",\n",
        "    \"httpd\": \"unknown\",\n",
        "    \"brian\": \"unknown\",\n",
        "    \"katie\": \"unknown\",\n",
        "    \"3\": \"unknown\",\n",
        "    \"ss\": \"unknown\",\n",
        "    \"map\": \"unknown\",\n",
        "    \"stereograms\": \"unknown\",\n",
        "    \"f94\": \"unknown\",\n",
        "    \"il\": \"unknown\"\n",
        "}\n",
        "\n",
        "# Step 3: Create Spark map expression from corrections\n",
        "mapping_expr = create_map([lit(x) for x in chain(*corrections.items())])\n",
        "\n",
        "# Step 4: Add cleaned column to DataFrame\n",
        "logs_df = logs_df.withColumn(\"file_extension_clean\",when(col(\"file_extension\").isin(valid_extensions), col(\"file_extension\"))\n",
        "    .otherwise(mapping_expr[col(\"file_extension\")]))\n",
        "\n",
        "## Replace nulls with 'unknown' after applying corrections\n",
        "logs_df = logs_df.withColumn(\n",
        "    \"file_extension_clean\",\n",
        "    when(col(\"file_extension_clean\").isNull(), \"unknown\").otherwise(col(\"file_extension_clean\")))\n",
        "\n",
        "logs_df = logs_df.drop(\"file_extension\").withColumnRenamed(\"file_extension_clean\", \"file_extension\")"
      ],
      "metadata": {
        "id": "6_5wdC53qPN9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULg1bRd0dd-h",
        "outputId": "a38aa1b1-03b3-4dea-fe45-6d3c3ddef99d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- host: string (nullable = true)\n",
            " |-- method: string (nullable = true)\n",
            " |-- endpoint: string (nullable = true)\n",
            " |-- status: integer (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- content_size: integer (nullable = true)\n",
            " |-- file_extension: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File extension distribution\n",
        "logs_df.groupBy(\"file_extension\").count().orderBy(\"count\", ascending=False).show(20, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ugiuul6vI0q",
        "outputId": "93ff0198-f93e-49ce-edf8-6cf9ba738c04"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------+\n",
            "|file_extension|count |\n",
            "+--------------+------+\n",
            "|html          |341555|\n",
            "|gif           |298799|\n",
            "|jpg           |42238 |\n",
            "|unknown       |29133 |\n",
            "|ps            |6265  |\n",
            "|txt           |3142  |\n",
            "|jpeg          |2681  |\n",
            "|qt            |1718  |\n",
            "|rgb           |695   |\n",
            "|htm           |238   |\n",
            "|gz            |111   |\n",
            "|tiff          |70    |\n",
            "|bmp           |48    |\n",
            "|pdf           |25    |\n",
            "|pcx           |8     |\n",
            "|zip           |6     |\n",
            "|doc           |3     |\n",
            "|tar           |3     |\n",
            "|wav           |1     |\n",
            "+--------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Min and Max timestamps\n",
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "timestamps = logs_df.select(min(\"timestamp\"), max(\"timestamp\")).collect()[0]\n",
        "print(f\"Timestamp range: {timestamps[0]} to {timestamps[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oUGgAuaBob2",
        "outputId": "9437587d-32e2-4ec7-874d-2e3a38fc00cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp range: 1994-10-24 19:41:41 to 1995-10-11 20:14:17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HTTP methods and counts\n",
        "from pyspark.sql.functions import when, col, trim\n",
        "\n",
        "logs_df = logs_df.withColumn(\"method\",when(trim(col(\"method\")) == \"\", \"GET\").otherwise(col(\"method\")))\n",
        "\n",
        "logs_df.groupBy(\"method\").count().orderBy(\"count\", ascending=False).show(25, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qyjW1p7xW_l",
        "outputId": "335e4142-0e16-4be0-8cee-bf2bd2855ecd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|method|count |\n",
            "+------+------+\n",
            "|GET   |726039|\n",
            "|HEAD  |597   |\n",
            "|POST  |103   |\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Status codes distribution\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "logs_df = logs_df.withColumn(\"status\",when(col(\"status\").isNull(), 200).otherwise(col(\"status\")))\n",
        "\n",
        "logs_df.groupBy(\"status\").count().orderBy(\"count\", ascending=False).show(25, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EL_IdjIxW9i",
        "outputId": "226812a7-147a-414b-9ad3-21261e579620"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|status|count |\n",
            "+------+------+\n",
            "|200   |570161|\n",
            "|304   |97796 |\n",
            "|302   |30300 |\n",
            "|404   |23593 |\n",
            "|403   |4743  |\n",
            "|401   |46    |\n",
            "|501   |43    |\n",
            "|500   |42    |\n",
            "|400   |15    |\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set NULL values to 0 if status is 404, else median\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "median_size = logs_df.approxQuantile(\"content_size\", [0.5], 0.01)[0]\n",
        "\n",
        "logs_df = logs_df.withColumn(\"content_size\",when(col(\"content_size\").isNull() & (col(\"status\") == 404), 0)\n",
        "    .when(col(\"content_size\").isNull(), median_size).otherwise(col(\"content_size\")))\n",
        "\n",
        "logs_df = logs_df.withColumn(\"content_size\", col(\"content_size\").cast(IntegerType()))\n",
        "logs_df.groupBy(\"content_size\").count().orderBy(\"count\", ascending=False).show(25, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzKrcfRUaM5Y",
        "outputId": "3260824e-c0a7-4055-8db1-b3908b324712"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+\n",
            "|content_size|count |\n",
            "+------------+------+\n",
            "|0           |122700|\n",
            "|2032        |39371 |\n",
            "|2555        |21209 |\n",
            "|36403       |19229 |\n",
            "|2881        |12108 |\n",
            "|441         |7971  |\n",
            "|3185        |6860  |\n",
            "|7259        |5518  |\n",
            "|2877        |5352  |\n",
            "|479         |4577  |\n",
            "|31977       |4398  |\n",
            "|16384       |4371  |\n",
            "|3020        |4222  |\n",
            "|8192        |3931  |\n",
            "|768         |3755  |\n",
            "|147         |3730  |\n",
            "|1234        |3429  |\n",
            "|191         |3275  |\n",
            "|2868        |3269  |\n",
            "|1512        |3082  |\n",
            "|469         |3009  |\n",
            "|1567        |2974  |\n",
            "|906         |2699  |\n",
            "|506         |2649  |\n",
            "|1268        |2644  |\n",
            "+------------+------+\n",
            "only showing top 25 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, lit\n",
        "\n",
        "logs_df = logs_df.withColumn(\"endpoint\",when(col(\"endpoint\").isNull() | (col(\"endpoint\") == \"\"), lit(\"unknown\")).otherwise(col(\"endpoint\")))"
      ],
      "metadata": {
        "id": "nNRzRlCkgf3L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name in logs_df.columns:\n",
        "    print(f\"\\n📊 Unique values and counts for column: {col_name}\")\n",
        "    logs_df.groupBy(col_name).count().orderBy(\"count\", ascending=False).show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlpdk-v9xW7N",
        "outputId": "15f4ef97-d42c-4986-a4ec-0de7aa8ed265"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Unique values and counts for column: host\n",
            "+------+------+\n",
            "|host  |count |\n",
            "+------+------+\n",
            "|local |375132|\n",
            "|remote|351607|\n",
            "+------+------+\n",
            "\n",
            "\n",
            "📊 Unique values and counts for column: method\n",
            "+------+------+\n",
            "|method|count |\n",
            "+------+------+\n",
            "|GET   |726039|\n",
            "|HEAD  |597   |\n",
            "|POST  |103   |\n",
            "+------+------+\n",
            "\n",
            "\n",
            "📊 Unique values and counts for column: endpoint\n",
            "+----------+------+\n",
            "|endpoint  |count |\n",
            "+----------+------+\n",
            "|index.html|140252|\n",
            "|3.gif     |24006 |\n",
            "|2.gif     |23606 |\n",
            "|4.gif     |8018  |\n",
            "|244.gif   |5149  |\n",
            "|5.html    |5010  |\n",
            "|4097.gif  |4874  |\n",
            "|8870.jpg  |4493  |\n",
            "|6733.gif  |4278  |\n",
            "|8472.gif  |3843  |\n",
            "|8308.gif  |3478  |\n",
            "|8216.gif  |3254  |\n",
            "|8492.gif  |3072  |\n",
            "|8493.gif  |3014  |\n",
            "|93.gif    |2833  |\n",
            "|167.html  |2768  |\n",
            "|990.html  |2297  |\n",
            "|8333.gif  |2243  |\n",
            "|304.xbm   |2153  |\n",
            "|222.gif   |2113  |\n",
            "+----------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "📊 Unique values and counts for column: status\n",
            "+------+------+\n",
            "|status|count |\n",
            "+------+------+\n",
            "|200   |570161|\n",
            "|304   |97796 |\n",
            "|302   |30300 |\n",
            "|404   |23593 |\n",
            "|403   |4743  |\n",
            "|401   |46    |\n",
            "|501   |43    |\n",
            "|500   |42    |\n",
            "|400   |15    |\n",
            "+------+------+\n",
            "\n",
            "\n",
            "📊 Unique values and counts for column: timestamp\n",
            "+-------------------+-----+\n",
            "|timestamp          |count|\n",
            "+-------------------+-----+\n",
            "|NULL               |1614 |\n",
            "|1995-04-12 18:01:58|20   |\n",
            "|1995-03-03 07:04:44|12   |\n",
            "|1995-04-12 18:02:03|11   |\n",
            "|1995-05-18 09:44:58|11   |\n",
            "|1995-02-14 01:31:20|10   |\n",
            "|1995-02-24 02:09:10|10   |\n",
            "|1995-01-20 19:09:51|10   |\n",
            "|1995-06-28 23:43:32|10   |\n",
            "|1995-02-14 01:06:47|10   |\n",
            "|1995-03-28 00:28:35|10   |\n",
            "|1995-02-12 18:26:34|10   |\n",
            "|1995-02-08 22:30:30|9    |\n",
            "|1995-05-04 23:36:11|9    |\n",
            "|1995-01-13 22:13:10|9    |\n",
            "|1995-02-08 01:59:37|9    |\n",
            "|1995-01-12 19:33:54|9    |\n",
            "|1995-02-14 00:43:43|9    |\n",
            "|1995-02-14 00:49:29|9    |\n",
            "|1995-02-18 01:10:33|9    |\n",
            "+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "📊 Unique values and counts for column: content_size\n",
            "+------------+------+\n",
            "|content_size|count |\n",
            "+------------+------+\n",
            "|0           |122700|\n",
            "|2032        |39371 |\n",
            "|2555        |21209 |\n",
            "|36403       |19229 |\n",
            "|2881        |12108 |\n",
            "|441         |7971  |\n",
            "|3185        |6860  |\n",
            "|7259        |5518  |\n",
            "|2877        |5352  |\n",
            "|479         |4577  |\n",
            "|31977       |4398  |\n",
            "|16384       |4371  |\n",
            "|3020        |4222  |\n",
            "|8192        |3931  |\n",
            "|768         |3755  |\n",
            "|147         |3730  |\n",
            "|1234        |3429  |\n",
            "|191         |3275  |\n",
            "|2868        |3269  |\n",
            "|1512        |3082  |\n",
            "+------------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "📊 Unique values and counts for column: file_extension\n",
            "+--------------+------+\n",
            "|file_extension|count |\n",
            "+--------------+------+\n",
            "|html          |341555|\n",
            "|gif           |298799|\n",
            "|jpg           |42238 |\n",
            "|unknown       |29133 |\n",
            "|ps            |6265  |\n",
            "|txt           |3142  |\n",
            "|jpeg          |2681  |\n",
            "|qt            |1718  |\n",
            "|rgb           |695   |\n",
            "|htm           |238   |\n",
            "|gz            |111   |\n",
            "|tiff          |70    |\n",
            "|bmp           |48    |\n",
            "|pdf           |25    |\n",
            "|pcx           |8     |\n",
            "|zip           |6     |\n",
            "|doc           |3     |\n",
            "|tar           |3     |\n",
            "|wav           |1     |\n",
            "+--------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# missing value count per column\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "logs_df.select([sum(col(c).isNull().cast(\"int\")).alias(f\"{c}\") for c in logs_df.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmneyI8iWxJt",
        "outputId": "08dbad06-f546-44b5-a126-94735e8448f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+--------+------+---------+------------+--------------+\n",
            "|host|method|endpoint|status|timestamp|content_size|file_extension|\n",
            "+----+------+--------+------+---------+------------+--------------+\n",
            "|   0|     0|       0|     0|     1614|           0|             0|\n",
            "+----+------+--------+------+---------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80da5c6e"
      },
      "source": [
        "## Part 2: Analysis Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1) Count the total number of HTTP requests in the log file."
      ],
      "metadata": {
        "id": "PE0sevNhhlUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def total_log_records() -> int:\n",
        "    \"\"\"\n",
        "    Q1: Count of total log records.\n",
        "\n",
        "    Objective:\n",
        "        Determine the total number of HTTP log entries in the dataset.\n",
        "        Each line in the log file represents one HTTP request.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of log entries.\n",
        "    \"\"\"\n",
        "    # Count total rows in logs_df DataFrame\n",
        "    total_records = logs_df.count()\n",
        "    return total_records\n",
        "\n",
        "\n",
        "answer1 = total_log_records()\n",
        "print(\"Answer 1:\",answer1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh97p5vehmTI",
        "outputId": "ab920b1a-a620-4a4f-c5f4-2bba11b43062"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 1: 726739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2) Determine the number of distinct hosts (IP addresses or domain names) that accessed the server."
      ],
      "metadata": {
        "id": "DKdUlimXieJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_host_count() -> int:\n",
        "    \"\"\"\n",
        "    Q2: Count of unique hosts.\n",
        "\n",
        "    Objective:\n",
        "        Determine how many distinct hosts accessed the server.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of unique hosts.\n",
        "    \"\"\"\n",
        "    unique_hosts = logs_df.select(\"host\").distinct().count()\n",
        "    return unique_hosts\n",
        "\n",
        "answer2 = unique_host_count()\n",
        "print(\"Answer 2:\",answer2)\n",
        "logs_df.select(\"host\").distinct().show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3-UFiUVh83P",
        "outputId": "07fd10f9-827e-4ed2-de2f-042b8c76c0ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 2: 2\n",
            "+------+\n",
            "|host  |\n",
            "+------+\n",
            "|local |\n",
            "|remote|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3) For each date, count how many unique filenames were requested."
      ],
      "metadata": {
        "id": "ukMUjw8hixHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format, countDistinct, col\n",
        "\n",
        "def datewise_unique_filename_counts() -> dict[str, int]:\n",
        "    \"\"\"\n",
        "    Q3: Date-wise unique filename counts.\n",
        "\n",
        "    Objective:\n",
        "        For each date, count the number of unique filenames that accessed the server.\n",
        "        The date should be in 'dd-MMM-yyyy' format (e.g., '01-Jul-1995').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping each date to its count of unique filenames.\n",
        "              Example: {'01-Jul-1995': 123, '02-Jul-1995': 150}\n",
        "    \"\"\"\n",
        "    # Format the timestamp to desired date string format\n",
        "    date_df = logs_df.withColumn(\"date\", date_format(col(\"timestamp\"), \"dd-MMM-yyyy\"))\n",
        "\n",
        "    # Group by date and count distinct endpoints\n",
        "    grouped_df = date_df.groupBy(\"date\").agg(countDistinct(\"endpoint\").alias(\"unique_filenames\"))\n",
        "\n",
        "    # Collect the results to driver as dictionary\n",
        "    result = {row[\"date\"]: row[\"unique_filenames\"] for row in grouped_df.collect()}\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "answer3 = datewise_unique_filename_counts()\n",
        "print(\"Answer 3:\\n\",answer3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thlg6yB-iyGp",
        "outputId": "6e2177c3-57f9-42c5-bc3e-59323a77bc30"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 3:\n",
            " {'03-Nov-1994': 452, '11-Nov-1994': 307, '26-Feb-1995': 517, '08-Jan-1995': 137, '06-Oct-1995': 900, '24-Jan-1995': 406, '11-Aug-1995': 458, '06-Nov-1994': 200, '11-Feb-1995': 390, '21-Aug-1995': 548, '30-Sep-1995': 645, '06-Apr-1995': 778, '16-Jun-1995': 503, '14-Nov-1994': 345, '11-Apr-1995': 838, '04-Sep-1995': 312, '04-May-1995': 702, '08-Oct-1995': 519, '09-Nov-1994': 326, '25-Jun-1995': 660, '13-Sep-1995': 779, '17-Mar-1995': 490, '05-Aug-1995': 500, '02-Sep-1995': 310, '01-Mar-1995': 552, '31-Mar-1995': 1014, '19-Apr-1995': 633, '09-Jan-1995': 397, '17-Aug-1995': 529, '12-Sep-1995': 778, '08-Feb-1995': 632, '03-Jul-1995': 463, '15-Jun-1995': 511, '18-Mar-1995': 358, '18-Apr-1995': 425, '30-Jul-1995': 432, '24-Aug-1995': 550, '07-Dec-1994': 358, '26-Jan-1995': 372, '20-Sep-1995': 770, '17-Jun-1995': 381, '21-Mar-1995': 768, '23-Nov-1994': 342, '19-May-1995': 528, '05-Jul-1995': 647, '16-Sep-1995': 593, '11-Dec-1994': 164, '15-Apr-1995': 430, '25-Aug-1995': 586, '08-Jul-1995': 298, '13-Dec-1994': 374, '03-Aug-1995': 661, '31-Aug-1995': 478, '02-Apr-1995': 396, '31-May-1995': 611, '25-Sep-1995': 755, '02-Aug-1995': 788, '15-Aug-1995': 487, '10-Dec-1994': 217, '12-Dec-1994': 402, '05-Nov-1994': 277, '20-Dec-1994': 357, '09-Jun-1995': 440, '09-Dec-1994': 360, '07-Jul-1995': 447, '09-Aug-1995': 669, '12-Jan-1995': 457, '29-Mar-1995': 935, '11-Oct-1995': 795, '28-Jan-1995': 412, '30-Mar-1995': 769, '31-Dec-1994': 74, '19-Jul-1995': 536, '23-Jul-1995': 503, None: 2, '21-Jul-1995': 693, '19-Sep-1995': 742, '06-Dec-1994': 374, '18-Dec-1994': 324, '13-May-1995': 317, '30-Oct-1994': 224, '29-Sep-1995': 843, '01-Jan-1995': 82, '02-Dec-1994': 357, '19-Jun-1995': 578, '10-Sep-1995': 400, '14-Mar-1995': 942, '05-Jun-1995': 462, '23-Feb-1995': 642, '12-May-1995': 486, '24-Oct-1994': 161, '06-Jan-1995': 216, '24-Jul-1995': 578, '28-Dec-1994': 121, '18-Jun-1995': 370, '30-Nov-1994': 428, '22-Aug-1995': 596, '11-May-1995': 684, '06-Sep-1995': 475, '29-Apr-1995': 442, '17-Dec-1994': 338, '24-Dec-1994': 88, '16-Feb-1995': 676, '03-Mar-1995': 506, '25-Mar-1995': 570, '22-Sep-1995': 672, '05-Dec-1994': 295, '20-Jun-1995': 513, '03-Dec-1994': 152, '10-Nov-1994': 349, '28-Feb-1995': 532, '16-Aug-1995': 614, '09-Oct-1995': 676, '18-Nov-1994': 366, '31-Jan-1995': 530, '16-Apr-1995': 463, '18-Sep-1995': 637, '07-May-1995': 678, '21-May-1995': 276, '11-Jan-1995': 384, '08-Mar-1995': 655, '23-Aug-1995': 672, '24-Feb-1995': 587, '04-Jun-1995': 341, '17-Apr-1995': 453, '14-Jan-1995': 246, '04-Mar-1995': 377, '28-Jul-1995': 544, '29-Oct-1994': 285, '03-Oct-1995': 834, '07-Nov-1994': 282, '23-Jan-1995': 415, '24-Sep-1995': 514, '07-Feb-1995': 717, '10-Feb-1995': 812, '01-May-1995': 435, '09-Feb-1995': 666, '12-Jul-1995': 473, '07-Jan-1995': 158, '12-Mar-1995': 455, '14-Apr-1995': 456, '13-Jun-1995': 486, '17-Sep-1995': 463, '25-Jan-1995': 443, '08-Sep-1995': 705, '20-Feb-1995': 449, '13-Apr-1995': 638, '06-May-1995': 509, '14-May-1995': 263, '22-Nov-1994': 328, '30-Aug-1995': 614, '23-Apr-1995': 340, '13-Jan-1995': 424, '04-Feb-1995': 451, '29-May-1995': 417, '17-Feb-1995': 625, '11-Mar-1995': 526, '04-Oct-1995': 916, '24-Apr-1995': 460, '26-Jul-1995': 662, '22-Jun-1995': 623, '16-Nov-1994': 399, '03-Apr-1995': 813, '03-May-1995': 584, '12-Nov-1994': 163, '24-Nov-1994': 361, '06-Jun-1995': 635, '16-Jul-1995': 308, '21-Sep-1995': 833, '11-Jun-1995': 312, '04-Nov-1994': 417, '12-Jun-1995': 481, '02-Jul-1995': 365, '11-Sep-1995': 688, '04-Jul-1995': 494, '09-Sep-1995': 480, '16-Jan-1995': 425, '01-Sep-1995': 446, '22-Mar-1995': 743, '04-Jan-1995': 320, '09-May-1995': 717, '13-Nov-1994': 187, '15-Dec-1994': 262, '19-Feb-1995': 446, '27-Mar-1995': 778, '03-Feb-1995': 566, '27-Nov-1994': 229, '19-Dec-1994': 337, '13-Mar-1995': 703, '27-Apr-1995': 654, '28-Mar-1995': 758, '27-May-1995': 233, '02-Feb-1995': 620, '05-Sep-1995': 466, '16-Mar-1995': 794, '23-Jun-1995': 592, '01-Dec-1994': 244, '09-Apr-1995': 569, '12-Apr-1995': 846, '30-May-1995': 582, '29-Jan-1995': 287, '24-May-1995': 507, '09-Jul-1995': 215, '05-Jan-1995': 323, '07-Aug-1995': 593, '29-Dec-1994': 174, '16-May-1995': 466, '11-Jul-1995': 582, '01-Oct-1995': 585, '18-Jul-1995': 536, '27-Jan-1995': 479, '27-Feb-1995': 491, '20-Apr-1995': 658, '01-Feb-1995': 581, '05-Apr-1995': 830, '22-May-1995': 413, '12-Feb-1995': 445, '24-Jun-1995': 287, '27-Aug-1995': 382, '31-Jul-1995': 622, '08-Nov-1994': 338, '20-Nov-1994': 194, '10-Oct-1995': 881, '20-Jul-1995': 577, '06-Mar-1995': 701, '07-Sep-1995': 614, '30-Apr-1995': 293, '10-Jan-1995': 354, '05-Mar-1995': 409, '29-Jun-1995': 494, '03-Jan-1995': 258, '17-May-1995': 493, '21-Jun-1995': 609, '08-Dec-1994': 371, '02-Jan-1995': 128, '09-Mar-1995': 692, '26-Mar-1995': 523, '21-Apr-1995': 750, '27-Sep-1995': 819, '19-Aug-1995': 340, '19-Nov-1994': 323, '01-Aug-1995': 677, '13-Aug-1995': 401, '28-May-1995': 224, '26-Oct-1994': 317, '05-May-1995': 531, '08-Aug-1995': 664, '22-Dec-1994': 278, '05-Feb-1995': 535, '10-Jun-1995': 361, '25-Dec-1994': 47, '28-Sep-1995': 922, '26-Aug-1995': 412, '28-Oct-1994': 400, '18-Aug-1995': 542, '07-Oct-1995': 535, '15-Mar-1995': 804, '08-Apr-1995': 619, '10-Apr-1995': 776, '26-Dec-1994': 109, '30-Jan-1995': 547, '08-Jun-1995': 649, '22-Apr-1995': 443, '21-Feb-1995': 504, '04-Dec-1994': 212, '26-Sep-1995': 809, '31-Oct-1994': 351, '06-Feb-1995': 572, '20-May-1995': 312, '24-Mar-1995': 637, '01-Jun-1995': 569, '15-Nov-1994': 314, '07-Apr-1995': 723, '13-Jul-1995': 536, '17-Jul-1995': 534, '02-May-1995': 695, '07-Jun-1995': 524, '15-Jan-1995': 219, '02-Mar-1995': 677, '10-Mar-1995': 773, '10-Jul-1995': 455, '01-Nov-1994': 437, '14-Jun-1995': 619, '21-Jan-1995': 294, '01-Jul-1995': 406, '22-Jul-1995': 372, '10-Aug-1995': 654, '25-Oct-1994': 331, '28-Nov-1994': 309, '27-Dec-1994': 158, '30-Dec-1994': 131, '19-Mar-1995': 485, '14-Jul-1995': 517, '02-Oct-1995': 841, '23-Dec-1994': 179, '10-May-1995': 744, '14-Sep-1995': 700, '19-Jan-1995': 454, '03-Jun-1995': 400, '04-Apr-1995': 862, '16-Dec-1994': 339, '08-May-1995': 771, '25-Apr-1995': 622, '22-Jan-1995': 259, '05-Oct-1995': 866, '17-Nov-1994': 409, '15-Feb-1995': 785, '25-Feb-1995': 341, '02-Jun-1995': 549, '06-Jul-1995': 488, '20-Aug-1995': 399, '21-Nov-1994': 351, '28-Aug-1995': 561, '29-Nov-1994': 420, '23-May-1995': 575, '30-Jun-1995': 457, '17-Jan-1995': 358, '15-May-1995': 569, '18-Feb-1995': 514, '01-Apr-1995': 407, '18-Jan-1995': 411, '14-Feb-1995': 703, '27-Jul-1995': 619, '27-Oct-1994': 434, '15-Jul-1995': 411, '03-Sep-1995': 210, '29-Jul-1995': 418, '13-Feb-1995': 648, '06-Aug-1995': 453, '12-Aug-1995': 356, '26-Nov-1994': 235, '25-May-1995': 462, '23-Sep-1995': 547, '14-Dec-1994': 341, '22-Feb-1995': 642, '28-Apr-1995': 571, '27-Jun-1995': 543, '02-Nov-1994': 425, '07-Mar-1995': 734, '26-May-1995': 522, '18-May-1995': 517, '29-Aug-1995': 521, '23-Mar-1995': 804, '26-Apr-1995': 619, '15-Sep-1995': 727, '25-Nov-1994': 337, '28-Jun-1995': 616, '14-Aug-1995': 607, '20-Jan-1995': 534, '20-Mar-1995': 520, '25-Jul-1995': 565, '04-Aug-1995': 741, '21-Dec-1994': 265, '26-Jun-1995': 549}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4) Number of 404 response codes"
      ],
      "metadata": {
        "id": "3OT6nPOYjTZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many HTTP requests resulted in a 404 (Not Found) response.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def count_404_errors() -> int:\n",
        "    \"\"\"\n",
        "    Q4: Number of 404 response codes.\n",
        "\n",
        "    Objective:\n",
        "        Count how many times the HTTP 404 Not Found status appears in the logs.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of 404 errors.\n",
        "    \"\"\"\n",
        "    count_404 = logs_df.filter(col(\"status\") == 404).count()\n",
        "    return count_404\n",
        "\n",
        "answer4 = count_404_errors()\n",
        "print(\"Answer 4:\",answer4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgbqADSRV473",
        "outputId": "9f3e0f22-3f97-45d8-8358-96591c825370"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 4: 23593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5)  Top 15 filenames with 404 responses"
      ],
      "metadata": {
        "id": "L2T_ptD-mvil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the 15 most requested URLs that resulted in a 404 error, sorted by frequency.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def top_15_filenames_with_404() -> list[tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Q5: Top 15 filenames with 404 responses.\n",
        "\n",
        "    Objective:\n",
        "        Identify which requested URLs most frequently resulted in a 404 error.\n",
        "        Return the top 15 filenames sorted by frequency.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
        "              Example: [('index.html', 200), ...]\n",
        "    \"\"\"\n",
        "    top_404_df = (\n",
        "        logs_df\n",
        "        .filter(col(\"status\") == 404)\n",
        "        .groupBy(\"endpoint\")\n",
        "        .count()\n",
        "        .orderBy(col(\"count\").desc())\n",
        "        .limit(15)\n",
        "    )\n",
        "\n",
        "    # Collect results into a Python list of tuples\n",
        "    result = [(row[\"endpoint\"], row[\"count\"]) for row in top_404_df.collect()]\n",
        "    return result\n",
        "\n",
        "\n",
        "answer5 = top_15_filenames_with_404()\n",
        "print(\"Answer 5:\\n\",answer5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq6ENvuEmdmu",
        "outputId": "8aae26e9-2de4-4f8e-a0a1-fbed8e68fa35"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 5:\n",
            " [('index.html', 4737), ('4115.html', 902), ('1611.html', 649), ('5698.xbm', 585), ('710.txt', 408), ('2002.html', 259), ('2177.gif', 193), ('10695.ps', 161), ('6555.html', 153), ('487.gif', 152), ('151.html', 149), ('488.gif', 148), ('40.html', 148), ('3414.gif', 148), ('9678.gif', 142)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6) Top 15 file extension with 404 responses"
      ],
      "metadata": {
        "id": "3gCy5aqbm_PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "def top_15_ext_with_404() -> list[tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Q6: Top 15 file extensions with 404 responses.\n",
        "\n",
        "    Objective:\n",
        "        Find which file extensions generated the most 404 errors.\n",
        "        Return the top 15 sorted by number of 404s.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (extension, count), sorted by count in descending order.\n",
        "              Example: [('html', 45), ...]\n",
        "    \"\"\"\n",
        "    top_404_ext_df = (logs_df.filter(col(\"status\") == 404).groupBy(\"file_extension\").count()\n",
        "        .orderBy(col(\"count\").desc()).limit(15))\n",
        "\n",
        "    result = [(row[\"file_extension\"], row[\"count\"]) for row in top_404_ext_df.collect()]\n",
        "    return result\n",
        "\n",
        "\n",
        "answer6 = top_15_ext_with_404()\n",
        "print(\"Answer 6:\\n\",answer6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64vkIGDhm_7_",
        "outputId": "836d25fe-895b-4810-db34-90774b5ffe8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 6:\n",
            " [('html', 12193), ('gif', 7205), ('unknown', 2209), ('ps', 754), ('jpg', 527), ('txt', 496), ('htm', 108), ('bmp', 28), ('rgb', 21), ('jpeg', 19), ('pcx', 8), ('gz', 7), ('pdf', 6), ('tiff', 6), ('tar', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7) Total bandwidth transferred per day for the month of July 1995"
      ],
      "metadata": {
        "id": "vjgd5Y10n5ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, sum, year, month, date_format\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assuming Spark session and logs_df are already defined outside the function\n",
        "\n",
        "def total_bandwidth_per_day() -> dict[str, int]:\n",
        "    \"\"\"\n",
        "    Q7: Total bandwidth transferred per day for the month of July 1995.\n",
        "\n",
        "    Objective:\n",
        "        Sum the number of bytes transferred per day.\n",
        "        Skip entries where the byte field is missing or '-'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping each date to total bytes transferred.\n",
        "              Example: {'01-Jul-1995': 123456789, ...}\n",
        "    \"\"\"\n",
        "    # Cast content_size to Integer (handle '-' or bad data outside this)\n",
        "    df_clean = logs_df.withColumn(\"content_size\", col(\"content_size\").cast(IntegerType()))\n",
        "\n",
        "    # Filter for July 1995 and non-null content_size\n",
        "    july_df = df_clean.filter(\n",
        "        (col(\"content_size\").isNotNull()) &\n",
        "        (year(\"timestamp\") == 1995) &\n",
        "        (month(\"timestamp\") == 7)\n",
        "    )\n",
        "\n",
        "    # Extract date formatted as dd-MMM-yyyy (e.g. 01-Jul-1995)\n",
        "    july_df = july_df.withColumn(\"date_formatted\", date_format(\"timestamp\", \"dd-MMM-yyyy\"))\n",
        "\n",
        "    # Group by formatted date and sum content_size\n",
        "    daily_bandwidth_df = july_df.groupBy(\"date_formatted\") \\\n",
        "        .agg(sum(\"content_size\").alias(\"total_bandwidth\")) \\\n",
        "        .orderBy(\"date_formatted\")\n",
        "\n",
        "    # Convert to dict[str, int]\n",
        "    bandwidth_dict = {row[\"date_formatted\"]: row[\"total_bandwidth\"]\n",
        "        for row in daily_bandwidth_df.collect()}\n",
        "\n",
        "    return bandwidth_dict\n",
        "\n",
        "\n",
        "\n",
        "answer7 = total_bandwidth_per_day()\n",
        "print(\"Answer 7:\")\n",
        "print(answer7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI212npD82rr",
        "outputId": "25fd3088-6c1e-4aaf-c2e2-3ee0fac14686"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 7:\n",
            "{'01-Jul-1995': 17176718, '02-Jul-1995': 7964456, '03-Jul-1995': 11848912, '04-Jul-1995': 25281666, '05-Jul-1995': 22616402, '06-Jul-1995': 20594119, '07-Jul-1995': 9656497, '08-Jul-1995': 5538242, '09-Jul-1995': 4359408, '10-Jul-1995': 13323182, '11-Jul-1995': 22872167, '12-Jul-1995': 17997766, '13-Jul-1995': 16065902, '14-Jul-1995': 16292286, '15-Jul-1995': 17997640, '16-Jul-1995': 8217684, '17-Jul-1995': 18621348, '18-Jul-1995': 18104512, '19-Jul-1995': 16314394, '20-Jul-1995': 25673588, '21-Jul-1995': 26150302, '22-Jul-1995': 6390566, '23-Jul-1995': 10241345, '24-Jul-1995': 20728586, '25-Jul-1995': 23626657, '26-Jul-1995': 26882549, '27-Jul-1995': 23100954, '28-Jul-1995': 37632768, '29-Jul-1995': 16393207, '30-Jul-1995': 21269643, '31-Jul-1995': 29983159}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dc00908"
      },
      "source": [
        "### Q8) Hourly request distribution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many HTTP requests occurred during each hour (0–23).\n",
        "\n",
        "from pyspark.sql.functions import hour, col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def hourly_request_distribution() -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Q8: Hourly request distribution.\n",
        "\n",
        "    Objective:\n",
        "        Count the number of requests made during each hour (00 to 23).\n",
        "        Useful for understanding traffic peaks.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping hour (int) to request count.\n",
        "              Example: {0: 120, 1: 90, ..., 23: 80}\n",
        "    \"\"\"\n",
        "\n",
        "     # Filter out rows with null timestamps to avoid None keys\n",
        "    filtered_df = logs_df.filter(col(\"timestamp\").isNotNull())\n",
        "\n",
        "    # Extract hour from timestamp\n",
        "    hourly_df = filtered_df.withColumn(\"hour\", hour(col(\"timestamp\")))\n",
        "\n",
        "    # Group by hour and count requests\n",
        "    hourly_counts_df = hourly_df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
        "\n",
        "    # Convert to dictionary {hour: count}\n",
        "    hourly_dict = {row[\"hour\"]: row[\"count\"] for row in hourly_counts_df.collect()}\n",
        "\n",
        "    return hourly_dict\n",
        "\n",
        "\n",
        "answer8 = hourly_request_distribution()\n",
        "print(\"Answer 8:\\n\")\n",
        "print(answer8)\n"
      ],
      "metadata": {
        "id": "EgxiNNZwmMgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e329af-1f3e-42e8-c968-a2d4535f1011"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 8:\n",
            "\n",
            "{0: 39619, 1: 32690, 2: 30750, 3: 28184, 4: 26032, 5: 22860, 6: 19874, 7: 17081, 8: 13892, 9: 11447, 10: 10588, 11: 10439, 12: 12325, 13: 15200, 14: 22101, 15: 30932, 16: 38039, 17: 46336, 18: 45779, 19: 50070, 20: 51183, 21: 52938, 22: 50530, 23: 46236}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "363b7083"
      },
      "source": [
        "### Q9) Top 10 most requested filenames"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the top 10 most frequently requested filenames,regardless of status code.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def top_10_most_requested_filenames() -> list[tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Q9: Top 10 most requested filenames.\n",
        "\n",
        "    Objective:\n",
        "        Identify the most commonly requested URLs (irrespective of status code).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
        "              Example: [('index.html', 500), ...]\n",
        "    \"\"\"\n",
        "\n",
        "    # Group by endpoint (filename) and count requests\n",
        "    filename_counts_df = logs_df.groupBy(\"endpoint\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "    # Take top 10 results and convert to list of tuples\n",
        "    top_10_list = [(row[\"endpoint\"], row[\"count\"]) for row in filename_counts_df.take(10)]\n",
        "\n",
        "    return top_10_list\n",
        "\n",
        "\n",
        "answer9 = top_10_most_requested_filenames()\n",
        "print(\"Answer 9:\")\n",
        "print(answer9)\n"
      ],
      "metadata": {
        "id": "vFRSqKnWmMdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb2e908-9226-4431-b506-9b48a6660304"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 9:\n",
            "[('index.html', 140252), ('3.gif', 24006), ('2.gif', 23606), ('4.gif', 8018), ('244.gif', 5149), ('5.html', 5010), ('4097.gif', 4874), ('8870.jpg', 4493), ('6733.gif', 4278), ('8472.gif', 3843)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eedb4778"
      },
      "source": [
        "### Q10) HTTP response code distribution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each HTTP response status code (e.g., 200, 404).\n",
        "\n",
        "def response_code_distribution() -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Q10: HTTP response code distribution.\n",
        "\n",
        "    Objective:\n",
        "        Count how often each HTTP status code appears in the logs.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping HTTP status codes (as int) to their frequency.\n",
        "              Example: {200: 150000, 404: 3000}\n",
        "    \"\"\"\n",
        "\n",
        "    # Cast status to integer (handle any bad data)\n",
        "    df_clean = logs_df.withColumn(\"status\", col(\"status\").cast(IntegerType()))\n",
        "\n",
        "    # Filter out rows with null status to avoid None keys\n",
        "    df_filtered = logs_df.filter(col(\"status\").isNotNull())\n",
        "\n",
        "    # Group by status and count occurrences\n",
        "    status_counts_df = df_filtered.groupBy(\"status\").count().orderBy(\"status\")\n",
        "\n",
        "    # Convert to dict {status_code: count}\n",
        "    status_dict = {row[\"status\"]: row[\"count\"] for row in status_counts_df.collect()}\n",
        "\n",
        "    return status_dict\n",
        "\n",
        "\n",
        "answer10 = response_code_distribution()\n",
        "print(\"Answer 10:\")\n",
        "print(answer10)"
      ],
      "metadata": {
        "id": "GX4wgolsUGD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6976933-1dff-4092-e6ed-cdb024cb6281"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 10:\n",
            "{200: 570161, 302: 30300, 304: 97796, 400: 15, 401: 46, 403: 4743, 404: 23593, 500: 42, 501: 43}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "--------------"
      ],
      "metadata": {
        "id": "rq8B8aGNjwTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Mrudula A P**\n",
        "\n",
        "Youtube Link : https://youtu.be/K1VvCuUy9w0"
      ],
      "metadata": {
        "id": "HeCC2dynAerg"
      }
    }
  ]
}